# MLPerf benchmark tasks

Here we describe how to automate MLPerf inference benchmark with the help 
of the [CK workflow framework](https://github.com/ctuning/ck) and native MLPerf scripts.

* [Image classification](task-image-classification.md)
* [Object detection](task-object-detection.md)
* [Medical imaging ](task-medical-imaging.md)
* [NLP](task-nlp.md)
* [Recommendation](task-recommendation.md)
* [Speech recognition](task-speech-recognition.md)





# Unsorted notes

* [Notes about datasets](../datasets/README.md)
* [Notes about models (issues, quantization, etc)](../models/notes.md)

* DLRM: [notes](dlrm.md), [CK packages](https://cknowledge.io/?q=module_uoa%3A%22program%22+AND+dlrm), [CK workflows](https://cknowledge.io/?q=module_uoa%3A%22program%22+AND+dlrm)
* [Search for CK program workflows with "mlperf"](https://cknowledge.io/?q=module_uoa%3A%22program%22+AND+mlperf)
* [Search for CK program workflows with "loadgen"](https://cknowledge.io/?q=module_uoa%3A%22program%22+AND+loadgen)

# Feedback
Ð¡ontact [Grigori Fursin](https://cKnowledge.io/@gfursin) ([OctoML.ai](https://octoml.ai), [MLCommons member](https://mlcommons.org))
