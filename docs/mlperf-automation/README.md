# MLPerf inference benchmark automation guide

This document is prepared by the [MLCommons community](https://mlcommons.org)
to make it easier to reproduce MLPerf benchmark results and automate new submissions
using the open-source [CK workflow framework](https://github.com/ctuning/ck).

* [Prepare your platform](platform/README.md)
* [Install CK framework](tools/ck.md)
  * [Install CK virtual environment (optional)](tools/ck-venv.md)
  * [Use adaptive CK container](tools/ck-docker.md)
* [Prepare and run native MLPerf](tasks/README.md)
* [Analyze MLPerf inference results](results/README.md)
  * [Example of CK dashboards for ML Systems DSE](results/ck-dashboard.md)
* [Reproduce MLPerf results and DSE](reproduce/README.md)
* [Test models with a webcam](reproduce/demo-webcam-object-detection-x86-64.md)
* [Explore ML Systems designs](dse/README.md)
* [Submit to MLPerf](submit/README.md)
* [Related tools](tools/README.md)
* Further improvements:
  * [Standardization of MLPerf workflows](tbd/standardization.md)
  * [More automation](tbd/automation.md)
  * [CK2 ideas](tbd/ck2.md)

# Feedback

Contact [Grigori Fursin](https://cKnowledge.io/@gfursin) ([OctoML.ai](https://octoml.ai), [MLCommons member](https://mlcommons.org))

