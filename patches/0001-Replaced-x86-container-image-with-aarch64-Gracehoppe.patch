From ca90f3a46225c91e601da84ef53709cdb209c6b3 Mon Sep 17 00:00:00 2001
From: Leonard226 <schmidtleonard74@gmail.com>
Date: Mon, 4 Nov 2024 16:20:34 +0100
Subject: [PATCH] Replaced x86 container image with aarch64-Gracehopper image

---
 patch.sh                                      | 20 +++++++++++++++++++
 script/app-mlperf-inference/_cm.yaml          |  6 +++---
 .../_cm.yaml                                  |  2 +-
 3 files changed, 24 insertions(+), 4 deletions(-)
 create mode 100755 patch.sh

diff --git a/patch.sh b/patch.sh
new file mode 100755
index 000000000..ae58b3141
--- /dev/null
+++ b/patch.sh
@@ -0,0 +1,20 @@
+#!/bin/bash
+
+# Aarch64 Gracehopper Image to use 
+NEW_IMAGE="nvcr.io/nvidia/mlperf/mlperf-inference:mlpinf-v4.1-cuda12.4-pytorch24.04-ubuntu22.04-aarch64-GraceHopper-release"
+
+CURR_DIR=$PWD
+RUN_DIR="${CURR_DIR}/script"
+
+cd "$RUN_DIR" || { echo "Directory $RUN_DIR not found."; exit 1; }
+
+# Find all relevant files containing x86 MLPerf images and replace them with the new aarch64 image
+grep -rl "nvcr.io/nvidia/mlperf/mlperf-inference" . | while read -r file; do
+    sed -i.bak "/nvcr.io\/nvidia\/mlperf\/mlperf-inference/ {
+        /x86_64/ s|nvcr.io/nvidia/mlperf/mlperf-inference:[^ ]*|$NEW_IMAGE|g
+    }" "$file"
+    echo "Updated MLPerf x86 image in: $file"
+done
+
+find . -name "*.bak" -type f -delete
+
diff --git a/script/app-mlperf-inference/_cm.yaml b/script/app-mlperf-inference/_cm.yaml
index 928709a8f..6a9d96d26 100644
--- a/script/app-mlperf-inference/_cm.yaml
+++ b/script/app-mlperf-inference/_cm.yaml
@@ -308,7 +308,7 @@ variations:
 
   nvidia-original,r4.1-dev_default:
     docker:
-      base_image: nvcr.io/nvidia/mlperf/mlperf-inference:mlpinf-v4.0-cuda12.2-cudnn8.9-x86_64-ubuntu20.04-public
+      base_image: nvcr.io/nvidia/mlperf/mlperf-inference:mlpinf-v4.1-cuda12.4-pytorch24.04-ubuntu22.04-aarch64-GraceHopper-release
       image_name: mlperf-inference-nvidia-v4.1-dev-common
 
   nvidia-original,r4.1-dev_default,gptj_:
@@ -321,7 +321,7 @@ variations:
 
   nvidia-original,r4.1_default:
     docker:
-      base_image: nvcr.io/nvidia/mlperf/mlperf-inference:mlpinf-v4.1-cuda12.4-pytorch24.04-ubuntu22.04-x86_64-release
+      base_image: nvcr.io/nvidia/mlperf/mlperf-inference:mlpinf-v4.1-cuda12.4-pytorch24.04-ubuntu22.04-aarch64-GraceHopper-release
 
   nvidia-original,r4.1_default,gptj_:
     docker:
@@ -358,7 +358,7 @@ variations:
     docker:
       interactive: True
       extra_run_args: ' --runtime=nvidia --ulimit memlock=-1 --cap-add SYS_ADMIN --cap-add SYS_TIME --security-opt apparmor=unconfined --security-opt seccomp=unconfined'
-      base_image: nvcr.io/nvidia/mlperf/mlperf-inference:mlpinf-v3.1-cuda12.2-cudnn8.9-x86_64-ubuntu20.04-l4-public
+      base_image: nvcr.io/nvidia/mlperf/mlperf-inference:mlpinf-v4.1-cuda12.4-pytorch24.04-ubuntu22.04-aarch64-GraceHopper-release
       os: "ubuntu"
       os_version: "20.04"
       deps:
diff --git a/script/build-mlperf-inference-server-nvidia/_cm.yaml b/script/build-mlperf-inference-server-nvidia/_cm.yaml
index c5003f67c..7beac26d6 100644
--- a/script/build-mlperf-inference-server-nvidia/_cm.yaml
+++ b/script/build-mlperf-inference-server-nvidia/_cm.yaml
@@ -361,7 +361,7 @@ docker:
   real_run: False
   interactive: True
   os_version: '20.04'
-  base_image: nvcr.io/nvidia/mlperf/mlperf-inference:mlpinf-v3.1-cuda12.2-cudnn8.9-x86_64-ubuntu20.04-l4-public
+  base_image: nvcr.io/nvidia/mlperf/mlperf-inference:mlpinf-v4.1-cuda12.4-pytorch24.04-ubuntu22.04-aarch64-GraceHopper-release
   docker_input_mapping:
     imagenet_path: IMAGENET_PATH
     gptj_checkpoint_path: GPTJ_CHECKPOINT_PATH
-- 
2.43.5

