{
  "alias": "get-ml-model-llama2",
  "automation_alias": "script",
  "automation_uid": "5b4e0237da074764",
  "cache": true,
  "category": "AI/ML models",
  "env": {
    "CM_ML_MODEL_DATASET": "openorca",
    "CM_ML_MODEL_WEIGHT_TRANSFORMATIONS": "no"
  },
  "input_mapping": {
    "checkpoint": "LLAMA2_CHECKPOINT_PATH"
  },
  "docker": {
    "real_run": false
  },
  "new_env_keys": [
    "CM_ML_MODEL_*",
    "LLAMA2_CHECKPOINT_PATH",
    "CM_NVIDIA_TP_SIZE"
  ],
  "prehook_deps": [
    {
      "enable_if_env": {
        "CM_TMP_REQUIRE_DOWNLOAD": [
          "yes"
        ]
      },
      "env": {
      },
      "force_env_keys": [
        "CM_GIT_CHECKOUT_FOLDER"
      ],
      "names": [
        "hf-zoo"
      ],
      "tags": "get,ml-model,huggingface,zoo,_clone-repo",
      "extra_cache_tags": "llama2,llama-2"
    }
  ],
  "tags": [
    "get",
    "raw",
    "ml-model",
    "language-processing",
    "llama2",
    "llama2-70b",
    "text-summarization"
  ],
  "uid": "5db97be9f61244c6",
  "variations": {
    "meta-llama/Llama-2-70b-chat-hf": {
      "group": "huggingface-stub",
      "default": true,
      "env": {
        "CM_GIT_CHECKOUT_FOLDER": "Llama-2-70b-chat-hf",
        "CM_MODEL_ZOO_ENV_KEY": "LLAMA2"
      },
      "adr": {
        "hf-zoo": {
          "tags": "_model-stub.meta-llama/Llama-2-70b-chat-hf"
        }
      }
    },
    "meta-llama/Llama-2-7b-chat-hf": {
      "group": "huggingface-stub",
      "env": {
        "CM_GIT_CHECKOUT_FOLDER": "Llama-2-7b-chat-hf",
        "CM_MODEL_ZOO_ENV_KEY": "LLAMA2"
      },
      "adr": {
        "hf-zoo": {
          "tags": "_model-stub.meta-llama/Llama-2-7b-chat-hf"
        }
      }
    },
    "stub.#": {
      "group": "huggingface-stub",
      "env": {
        "CM_MODEL_ZOO_ENV_KEY": "LLAMA2"
      },
      "adr": {
        "hf-zoo": {
          "tags": "_model-stub.#"
        }
      }
    },
    "batch_size.#": {
      "env": {
        "CM_ML_MODEL_BATCH_SIZE": "#"
      }
    },
    "fp32": {
      "default": true,
      "env": {
        "CM_ML_MODEL_INPUT_DATA_TYPES": "fp32",
        "CM_ML_MODEL_PRECISION": "fp32",
        "CM_ML_MODEL_WEIGHT_DATA_TYPES": "fp32"
      },
      "group": "precision"
    },
    "tp-size.#": {
      "env": {
        "CM_NVIDIA_TP_SIZE": "#"
      },
      "group": "gpu"
    },
    "generic": {
      "env":{
        "CM_NVIDIA_TP_SIZE": 2
      },
      "group": "gpu"
    },
    "L40s": {
      "env":{
        "CM_NVIDIA_TP_SIZE": 4
      },
      "group": "gpu"
    },
    "fp8": {
      "env": {
        "CM_ML_MODEL_INPUT_DATA_TYPES": "fp8",
        "CM_ML_MODEL_PRECISION": "fp8",
        "CM_ML_MODEL_WEIGHT_DATA_TYPES": "fp8"
      },
      "group": "precision"
    },
    "pytorch": {
      "default": true,
      "env": {
        "CM_ML_MODEL_FRAMEWORK": "pytorch"
      },
      "group": "framework"
    },
    "amd": {
      "default_variations": {
        "framework": "pytorch",
        "precision": "fp8"
      },
      "group": "model-provider",
      "env": {
        "CM_TMP_ML_MODEL_PROVIDER": "amd"
      },
      "new_env_keys": [
        "CM_LLAMA2_FINAL_SAFE_TENSORS_ROOT",
        "CM_LLAMA2_FINAL_SAFE_TENSORS_PATH"
      ],
      "default_env": {
        "CM_LLAMA2_QUANTIZATION_DEVICE": ""
      }
    },
    "pytorch,amd": {
      "default_variations": {
        "precision": "fp8",
        "gpu": "generic"
      },
      "deps": [
        {
          "tags": "get,python3",
          "names": [
            "python",
            "python3"
          ]
        },
        {
          "tags": "get,ml-model,llama2-70b,_fp32,_pytorch",
          "env": {
          },
          "force_new_env_keys": [
            "LLAMA2_CHECKPOINT_PATH"
          ]
        },
        {
          "tags": "get,preprocessed,dataset,openorca,_calibration,_mlc"
        },
        {
          "tags": "get,git,repo,_repo.https://github.com/mlcommons/submissions_inference_v4.1",
          "extra_cache_tags": "inference,submissions",
          "env": {
            "CM_GIT_CHECKOUT_PATH_ENV_NAME": "CM_MLPERF_INFERENCE_RESULTS_PATH"
          }
        },
        {
          "tags": "get,generic-python-lib,_quark-amd"
        },
        {
          "tags": "get,generic-python-lib,_package.nltk"
        },
        {
          "tags": "get,generic-python-lib,_torch"
        }
      ]
    },
    "nvidia": {
      "default_variations": {
        "framework": "pytorch"
      },
      "group": "model-provider",
      "env": {
        "CM_TMP_ML_MODEL_PROVIDER": "nvidia"
      }
    },
    "pytorch,nvidia": {
      "default_variations": {
        "precision": "fp8",
        "gpu": "generic"
      },
      "deps": [
        {
          "tags": "get,git,repo,_repo.https://github.com/NVIDIA/TensorRT-LLM.git,_sha.0ab9d17a59c284d2de36889832fe9fc7c8697604",
          "extra_cache_tags": "tensorrt-llm",
          "env": {
            "CM_GIT_CHECKOUT_PATH_ENV_NAME": "CM_TENSORRT_LLM_CHECKOUT_PATH"
          }
        },
        {
          "tags": "get,cuda",
          "names": [
            "cuda"
          ]
        },
        {
          "tags": "get,nvidia,scratch,space"
        },
        {
          "tags": "get,cuda-devices"
        },
        {
          "tags": "get,ml-model,llama2-70b,_fp32,_pytorch",
          "env": {
          },
          "force_new_env_keys": [
            "LLAMA2_CHECKPOINT_PATH"
          ]
        },
        {
          "tags": "get,nvidia,inference,common-code",
          "names": [
            "nvidia-inference-common-code"
          ]
        },
        {
          "tags": "get,python3",
          "names": [
            "python",
            "python3"
          ]
        }
      ]
    },
    "int8": {
      "env": {
        "CM_ML_MODEL_INPUT_DATA_TYPES": "int8",
        "CM_ML_MODEL_PRECISION": "int8",
        "CM_ML_MODEL_WEIGHT_DATA_TYPES": "int8"
      },
      "group": "precision"
    },
    "pytorch,fp32": {
      "env": {}
    },
    "uint8": {
      "env": {
        "CM_ML_MODEL_INPUT_DATA_TYPES": "uint8",
        "CM_ML_MODEL_PRECISION": "uint8",
        "CM_ML_MODEL_WEIGHT_DATA_TYPES": "uint8"
      },
      "group": "precision"
    }
  },
  "print_env_at_the_end" : {
    "LLAMA2_CHECKPOINT_PATH": "LLAMA2 checkpoint path"
  }
}
